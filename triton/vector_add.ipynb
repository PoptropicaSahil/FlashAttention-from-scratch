{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will run on the GPU\n",
    "# GPU will only give the pointer to first element of tensor in memory, \n",
    "# then upto us for computing all the indices of elements that we want to access\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr,  # *Pointer* to the first input vector\n",
    "    y_ptr,  # *Pointer* to the second input vector\n",
    "    output_ptr,  # *Pointer* to the output vectorelement\n",
    "    n_elements,  # size of the vector\n",
    "    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process\n",
    "    # NOTE: 'constexpr' so it can be used as a shape value\n",
    "):\n",
    "    # There are multiple 'programs' processing different data. We identify which program \n",
    "    # This is analogous to the block id in CUDA\n",
    "    pid = tl.program_id(axis = 0) # We use 1D launch grid so axis is 0\n",
    "\n",
    "    # This program will process inputs that are offset from the initial data.\n",
    "    # For eg, if you had a vector of length 256 and block_size 64, then programs\n",
    "    # would each access the elements [0:64, 64:128, 128:192. 192:256].\n",
    "    # *Note that offsets is a list of pointers*\n",
    "\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE) # how to load the elements based on the pointer\n",
    "    # for pid 0 --> offset is 0, 1, 2, ..., 1023\n",
    "    # for pid 1 --> offset is 1024, 1025, 1026, ..., 2047\n",
    "    # for pid 2 --> offset is 2048, 2049, 2050, ..., 3071\n",
    "\n",
    "    # Create a mask to guard memeory operations agains out-of-bounds accesses\n",
    "    # We need a mask because n_elements may not be a multiple of BLOCK_SIZE\n",
    "    # So the last program (with the largest pid) cannot access all the elements\n",
    "    # in its block, so we need to mask out the loading of the elements only\n",
    "    # to those actually present in the tensor\n",
    "    # Eg. if we have 2060 elements, then pid 2 --> 2048, 2049, .. 2060, 2061, ...3071\n",
    "    # Mask ensures threads working after 2060 don't know anything \n",
    "    # i.e. of all offsets present, only work for those with value < n_elements\n",
    "    mask = offsets < n_elements\n",
    "    \n",
    "    # Load x and y from DRAM, masking out any extra elements in case the input is not  \n",
    "    # a multiple of BLOCK_SIZE\n",
    "    x = tl.load(x_ptr + offsets, mask = mask)\n",
    "    y = tl.load(y_ptr + offsets, mask = mask)\n",
    "    # in CUDA we did output[i] = x[i] + y[i], but here we do all at once\n",
    "    output = x + y # Shape: BLOCK_SIZE\n",
    "    \n",
    "    # Write x + y back to DRAM\n",
    "    tl.store(output_ptr + offsets, output, mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x: torch.Tensor, y: torch.Tensor): \n",
    "    # we need to preallocate the output\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "    n_elements = output.numel() # gives total number of elements in array\n",
    "\n",
    "    # The SPMD launch grid denotes the number of kernel instances that run in parallel\n",
    "    # It is analogous to CUDA launch grids. It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].\n",
    "    # In this case, we use a 1D grid where the size is the number of blocks\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), ) # does ceil(n_elements / meta['BLOCK_SIZE)\n",
    "\n",
    "    # NOTE:\n",
    "    # - Each torch.tensor object is implicitly converted into a pointer to its first element\n",
    "    # - triton.jit functions can be indexed with a launch grid to obtain a callable GPU kernel\n",
    "    # - Don't forget to pass meta-parameters as keywords argument\n",
    "\n",
    "    # What each block should do - is defined in the kernel\n",
    "    add_kernel[grid](x, y, output, n_elements=n_elements, BLOCK_SIZE=1024)\n",
    "    \n",
    "    # We return a handle to z but, since torch.cuda.synchronize() hasn't been called, the kernel\n",
    "    # is still running asynchronously at this point\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "size = 98432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size, device = 'cuda')\n",
    "y = torch.rand(size, device = 'cuda')\n",
    "output_torch = x + y\n",
    "output_triton = add(x, y)\n",
    "print(f\"output: {output_torch}, output_triton: {output_triton}\")\n",
    "print(\n",
    "    f\"The maximum difference between torch and triton is {torch.max(torch.abs(output_torch - output_triton))}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
